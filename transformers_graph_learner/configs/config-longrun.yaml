defaults:
  - override hydra/launcher: joblib

seed: 1

wandb:
  project: "transformer-graph-learner"
  entity: "L65_Project"
  group: "large_dataset_size"

dataset:
  num_graphs: 100000
  d_p: 16
  d_e: 1
  node_id_encode: "laplacian"
  in_feat_dim: 1
  split: 0.8
  n_nodes_range: [8, 32]


training:
  epoch: &num_epochs 1000
  num_epochs: *num_epochs
  batch_size: 128
  lr: 2e-4
  weight_decay: 0.1
  device: "cuda"  # set to "cpu" if you don't want to use CUDA
  save_every: 50

model:
  d_model: 512
  nhead: 16
  num_layers: 3
  dropout: 0.1
  input_dropout: 0.1

scheduler:
  warmup_steps: 300  # Check the num_epochs and adjust this value accordingly


paths:
  models: "models"
