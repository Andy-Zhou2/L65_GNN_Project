seed: 42

wandb:
  project: "transformer-graph-learner"

dataset:
  num_graphs: 10000
  d_p: 10
  d_e: 10
  in_feat_dim: 1
  split: 0.8
  n_nodes_range: [10, 10]


training:
  epoch: &num_epochs 300
  num_epochs: *num_epochs
  batch_size: 1024
  lr: 1e-3
  device: "cuda"  # set to "cpu" if you don't want to use CUDA
  save_every: 50

model:
  d_model: 512
  nhead: 8
  num_layers: 3

scheduler:
  factor: 0.1
  patience: 20
  T_max: *num_epochs
  eta_min: 1e-6

paths:
  models: "models"
