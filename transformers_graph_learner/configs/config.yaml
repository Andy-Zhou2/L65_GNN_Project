seed: 42

wandb:
  project: "transformer-graph-learner"

dataset:
  num_graphs: 1000
  d_p: 10
  d_e: 10
  in_feat_dim: 1
  split: 0.8


training:
  epoch: &num_epochs 2000
  num_epochs: *num_epochs
  batch_size: 1
  lr: 1e-3
  device: "cuda"  # set to "cpu" if you don't want to use CUDA
  save_every: 100

model:
  d_model: 128
  nhead: 8
  num_layers: 1

scheduler:
  factor: 0.1
  patience: 20
  T_max: *num_epochs
  eta_min: 1e-6

paths:
  models: "models"
