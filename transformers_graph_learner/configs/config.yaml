seed: 42

wandb:
  project: "transformer-graph-learner"

dataset:
  num_graphs: 1000
  d_p: 10
  d_e: 10
  in_feat_dim: 1
  split: 0.8

training:
  num_epochs: 300
  batch_size: 1
  lr: 1e-5
  device: "cuda"  # set to "cpu" if you don't want to use CUDA
  save_every: 50

model:
  d_model: 128
  nhead: 8
  num_layers: 4

scheduler:
  factor: 0.1
  patience: 5

paths:
  models: "models"
